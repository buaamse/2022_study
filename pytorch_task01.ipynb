{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electronic-baghdad",
   "metadata": {},
   "source": [
    "## 自动求导\n",
    "\n",
    "PyTorch 中，所有神经网络的核心是 `autograd `包。autograd包为张量上的所有操作提供了自动求导机制。它是一个在运行时定义 ( define-by-run ）的框架，这意味着反向传播是根据代码如何运行来决定的，并且每次迭代可以是不同的。\n",
    "\n",
    "`torch.Tensor `是这个包的核心类。如果设置它的属性` .requires_grad` 为 `True`，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用` .backward()`，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到`.grad`属性。\n",
    "\n",
    "注意：在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的Tensor。\n",
    "\n",
    "要阻止一个张量被跟踪历史，可以调用` .detach() `方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。为了防止跟踪历史记录(和使用内存），可以将代码块包装在 `with torch.no_grad(): `中。在评估模型时特别有用，因为模型可能具有 `requires_grad = True` 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。\n",
    "\n",
    "还有一个类对于`autograd`的实现非常重要：`Function`。`Tensor `和` Function` 互相连接生成了一个无环图 (acyclic graph)，它编码了完整的计算历史。每个张量都有一个` .grad_fn `属性，该属性引用了创建 `Tensor `自身的`Function`(除非这个张量是用户手动创建的，即这个张量的` grad_fn `是 `None` )。下面给出的例子中，张量由用户手动创建，因此grad_fn返回结果是None。\n",
    "\n",
    "```python\n",
    "x = torch.randn(3,3,requires_grad=True)\n",
    "print(x.grad_fn)\n",
    "```\n",
    "\n",
    "```\n",
    "None\n",
    "```\n",
    "\n",
    "如果需要计算导数，可以在 `Tensor` 上调用 `.backward()`。如果` Tensor` 是一个标量(即它包含一个元素的数据），则不需要为 `backward() `指定任何参数，但是如果它有更多的元素，则需要指定一个` gradient `参数，该参数是形状匹配的张量。\n",
    "\n",
    "```python\n",
    "import torch\n",
    "```\n",
    "\n",
    "创建一个张量并设置`requires_grad=True`用来追踪其计算历史\n",
    "\n",
    "```python\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([[1., 1.],\n",
    "        [1., 1.]], requires_grad=True)\n",
    "```\n",
    "\n",
    "对这个张量做一次运算：\n",
    "\n",
    "```python\n",
    "y = x**2\n",
    "print(y)\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([[1., 1.],\n",
    "        [1., 1.]], grad_fn=<PowBackward0>)\n",
    "```\n",
    "\n",
    "`y`是计算的结果，所以它有`grad_fn`属性。\n",
    "\n",
    "```python\n",
    "print(y.grad_fn)\n",
    "```\n",
    "\n",
    "```\n",
    "<PowBackward0 object at 0x7fb64d56cf70>\n",
    "```\n",
    "\n",
    "对 y 进行更多操作\n",
    "\n",
    "```python\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([[3., 3.],\n",
    "        [3., 3.]], grad_fn=<MulBackward0>) tensor(3., grad_fn=<MeanBackward0>)\n",
    "```\n",
    "\n",
    "`.requires_grad_(...) `原地改变了现有张量的` requires_grad `标志。如果没有指定的话，默认输入的这个标志是` False`。\n",
    "\n",
    "```python\n",
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)\n",
    "```\n",
    "\n",
    "```\n",
    "False\n",
    "True\n",
    "<SumBackward0 object at 0x7f1b24845f98>\n",
    "```\n",
    "\n",
    "**梯度**\n",
    "\n",
    "现在开始进行反向传播，因为` out` 是一个标量，因此` out.backward() `和` out.backward(torch.tensor(1.))` 等价。\n",
    "\n",
    "```python\n",
    "out.backward()\n",
    "```\n",
    "\n",
    "输出导数` d(out)/dx`\n",
    "\n",
    "```python\n",
    "print(x.grad)\n",
    "```\n",
    "\n",
    "数学上，若有向量函数$\\vec{y}=f(\\vec{x})$，那么 $\\vec{y}$ 关于 $\\vec{x}$ 的梯度就是一个雅可比矩阵：\n",
    "$$\n",
    "J=\\left(\\begin{array}{ccc}\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\\end{array}\\right)\n",
    "$$\n",
    "而 `torch.autograd` 这个包就是用来计算一些雅可比矩阵的乘积的。例如，如果 $v$ 是一个标量函数 $l = g(\\vec{y})$ 的梯度：\n",
    "$$\n",
    "v=\\left(\\begin{array}{lll}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)\n",
    "$$\n",
    "由链式法则，我们可以得到：\n",
    "$$\n",
    "v J=\\left(\\begin{array}{lll}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)\\left(\\begin{array}{ccc}\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\\end{array}\\right)=\\left(\\begin{array}{lll}\\frac{\\partial l}{\\partial x_{1}} & \\cdots & \\frac{\\partial l}{\\partial x_{n}}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。\n",
    "\n",
    "```python\n",
    "# 再来反向传播⼀一次，注意grad是累加的\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([[5.5000, 5.5000],\n",
    "        [5.5000, 5.5000]])\n",
    "tensor([[1., 1.],\n",
    "        [1., 1.]])\n",
    "```\n",
    "\n",
    "现在我们来看一个雅可比向量积的例子：\n",
    "\n",
    "```python\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x * 2\n",
    "i = 0\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    i = i + 1\n",
    "print(y)\n",
    "print(i)\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([-0.3464, -0.1451,  1.6939], requires_grad=True)\n",
    "tensor([-354.7274, -148.6218, 1734.5309], grad_fn=<MulBackward0>)\n",
    "9\n",
    "```\n",
    "\n",
    "在这种情况下，`y `不再是标量。`torch.autograd` 不能直接计算完整的雅可比矩阵，但是如果我们只想要雅可比向量积，只需将这个向量作为参数传给 `backward：`\n",
    "\n",
    "```python\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)\n",
    "```\n",
    "\n",
    "也可以通过将代码块包装在` with torch.no_grad():` 中，来阻止 autograd 跟踪设置了` .requires_grad=True `的张量的历史记录。\n",
    "\n",
    "```python\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)\n",
    "```\n",
    "\n",
    "```\n",
    "True\n",
    "True\n",
    "False\n",
    "```\n",
    "\n",
    "如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录(即不会影响反向传播)， 那么我们可以对 tensor.data 进行操作。\n",
    "\n",
    "```python\n",
    "x = torch.ones(1,requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值 \n",
    "print(x.grad)\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([1.])\n",
    "False\n",
    "tensor([100.], requires_grad=True)\n",
    "tensor([2.])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imported-commission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "x = torch.randn(3,3,requires_grad=True)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confused-diana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subtle-aruba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<PowBackward0>)\n",
      "<PowBackward0 object at 0x7fa8e0687550>\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<MulBackward0>) tensor(3., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beginning-legislature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1264,  0.9700],\n",
      "        [ 0.7808, -0.6376]])\n",
      "tensor([[ 26.7408, -97.0164],\n",
      "        [-10.6834,   1.1680]])\n",
      "False\n",
      "True\n",
      "tensor(10242.7510, grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x7fa868d11a30>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "print(a)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a)\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "disciplinary-discount",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4., grad_fn=<SumBackward0>)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor(4., grad_fn=<SumBackward0>)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 再来反向传播⼀一次，注意grad是累加的\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(out2)\n",
    "print(x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "print(out3)\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "surgical-dispatch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2743,  0.2623,  0.7204], requires_grad=True)\n",
      "tensor([-561.7030,  537.2490, 1475.3563], grad_fn=<MulBackward0>)\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 现在我们来看一个雅可比向量积的例子：\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x * 2\n",
    "i = 0\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    i = i + 1\n",
    "print(y)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "herbal-demographic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 在这种情况下，y不再是标量。torch.autograd 不能直接计算完整的雅可比矩阵，但是如果我们只想要雅可比向量积，只需将这个向量作为参数传给 backward：\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)\n",
    "\n",
    "# 也可以通过将代码块包装在with torch.no_grad(): 中，来阻止 autograd 跟踪设置了.requires_grad=True的张量的历史记录。\n",
    "\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "spiritual-theta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# 如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录(即不会影响反向传播)， 那么我们可以对 tensor.data 进行操作。\n",
    "\n",
    "x = torch.ones(1,requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值 \n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-saudi",
   "metadata": {},
   "source": [
    "# 并行计算简介\n",
    "\n",
    "在利用PyTorch做深度学习的过程中，可能会遇到数据量较大无法在单块GPU上完成，或者需要提升计算速度的场景，这时就需要用到并行计算。本节让我们来简单地了解一下并行计算的基本概念和主要实现方式，具体的内容会在课程的第二部分详细介绍。\n",
    "\n",
    "#### 2.3.1  为什么要做并行计算\n",
    "\n",
    "我们学习PyTorch的目的就是可以编写我们自己的框架，来完成特定的任务。可以说，在深度学习时代，GPU的出现让我们可以训练的更快，更好。所以，如何充分利用GPU的性能来提高我们模型学习的效果，这一技能是我们必须要学习的。这一节，我们主要讲的就是PyTorch的并行计算。PyTorch可以在编写完模型之后，让多个GPU来参与训练。\n",
    "\n",
    "#### 2.3.2  CUDA是个啥\n",
    "\n",
    "`CUDA`是我们使用GPU的提供商——NVIDIA提供的GPU并行计算框架。对于GPU本身的编程，使用的是`CUDA`语言来实现的。但是，在我们使用PyTorch编写深度学习代码时，使用的`CUDA`又是另一个意思。在PyTorch使用 `CUDA`表示要开始要求我们的模型或者数据开始使用GPU了。\n",
    "\n",
    "在编写程序中，当我们使用了 `cuda()` 时，其功能是让我们的模型或者数据迁移到GPU当中，通过GPU开始计算。\n",
    "\n",
    "#### 2.3.3  做并行的方法：\n",
    "\n",
    "- **网络结构分布到不同的设备中(Network partitioning)**\n",
    "\n",
    "在刚开始做模型并行的时候，这个方案使用的比较多。其中主要的思路是，将一个模型的各个部分拆分，然后将不同的部分放入到GPU来做不同任务的计算。其架构如下：\n",
    "\n",
    "![模型并行.png](./figures/模型并行.png)\n",
    "\n",
    "这里遇到的问题就是，不同模型组件在不同的GPU上时，GPU之间的传输就很重要，对于GPU之间的通信是一个考验。但是GPU的通信在这种密集任务中很难办到。所有这个方式慢慢淡出了视野，\n",
    "\n",
    "- **同一层的任务分布到不同数据中**(**Layer-wise partitioning**)\n",
    "\n",
    "第二种方式就是，同一层的模型做一个拆分，让不同的GPU去训练同一层模型的部分任务。其架构如下：\n",
    "\n",
    "![拆分.png](./figures/拆分.png)\n",
    "\n",
    "这样可以保证在不同组件之间传输的问题，但是在我们需要大量的训练，同步任务加重的情况下，会出现和第一种方式一样的问题。\n",
    "\n",
    "- **不同的数据分布到不同的设备中，执行相同的任务(Data parallelism)**\n",
    "\n",
    "第三种方式有点不一样，它的逻辑是，我不再拆分模型，我训练的时候模型都是一整个模型。但是我将输入的数据拆分。所谓的拆分数据就是，同一个模型在不同GPU中训练一部分数据，然后再分别计算一部分数据之后，只需要将输出的数据做一个汇总，然后再反传。其架构如下：\n",
    "\n",
    "![数据并行.png](./figures/数据并行.png)\n",
    "\n",
    "这种方式可以解决之前模式遇到的通讯问题。\n",
    "\n",
    "**PS:现在的主流方式是数据并行的方式(Data parallelism)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-berlin",
   "metadata": {},
   "source": [
    "基本配置\n",
    "\n",
    "首先导入必须的包。对于一个PyTorch项目，我们需要导入一些Python常用的包来帮助我们快速实现功能。常见的包有os、numpy等，此外还需要调用PyTorch自身一些模块便于灵活使用，比如torch、torch.nn、torch.utils.data.Dataset、torch.utils.data.DataLoader、torch.optimizer等等。注意这里只是建议导入的包导入的方式，可以采用不同的方案，比如涉及到表格信息的读入很可能用到pandas，对于不同的项目可能还需要导入一些更上层的包如cv2等。如果涉及可视化还会用到matplotlib、seaborn等。涉及到下游分析和指标计算也常用到sklearn。\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optimizer\n",
    "\n",
    "根据前面我们对深度学习任务的梳理，有如下几个超参数可以统一设置，方便后续调试时修改：\n",
    "\n",
    "    batch size\n",
    "    初始学习率（初始）\n",
    "    训练次数（max_epochs）\n",
    "    GPU配置\n",
    "\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "max_epochs = 100\n",
    "\n",
    "GPU的设置有两种常见的方式：\n",
    "\n",
    "#方案一：使用os.environ，这种情况如果使用GPU不需要设置\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "#方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "当然还会有一些其他模块或用户自定义模块会用到的参数，有需要也可以在一开始进行设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "confident-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torch.optim as optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-voluntary",
   "metadata": {},
   "source": [
    "### 数据读入\n",
    "\n",
    "PyTorch数据读入是通过Dataset+DataLoader的方式完成的，Dataset定义好数据的格式和数据变换形式，DataLoader用iterative的方式不断读入批次数据。\n",
    "\n",
    "我们可以定义自己的Dataset类来实现灵活的数据读取，定义的类需要继承PyTorch自身的Dataset类。主要包含三个函数：\n",
    "\n",
    "- `__init__`: 用于向类中传入外部参数，同时定义样本集\n",
    "- `__getitem__`: 用于逐个读取样本集合中的元素，可以进行一定的变换，并将返回训练/验证所需的数据\n",
    "- `__len__`: 用于返回数据集的样本数\n",
    "\n",
    "下面以cifar10数据集为例给出构建Dataset类的方式：\n",
    "\n",
    "```python\n",
    "train_data = datasets.ImageFolder(train_path, transform=data_transform)\n",
    "val_data = datasets.ImageFolder(val_path, transform=data_transform)\n",
    "```\n",
    "\n",
    "这里使用了PyTorch自带的ImageFolder类的用于读取按一定结构存储的图片数据（path对应图片存放的目录，目录下包含若干子目录，每个子目录对应属于同一个类的图片）。\n",
    "\n",
    "其中“data_transform”可以对图像进行一定的变换，如翻转、裁剪等操作，可自己定义。这里我们会在下一章通过实战加以介绍。\n",
    "\n",
    "这里另外给出一个例子，其中图片存放在一个文件夹，另外有一个csv文件给出了图片名称对应的标签。这种情况下需要自己来定义Dataset类：\n",
    "\n",
    "```python\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, info_csv, image_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: path to image directory.\n",
    "            info_csv: path to the csv file containing image indexes\n",
    "                with corresponding labels.\n",
    "            image_list: path to the txt file contains image names to training/validation set\n",
    "            transform: optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        label_info = pd.read_csv(info_csv)\n",
    "        image_file = open(image_list).readlines()\n",
    "        self.data_dir = data_dir\n",
    "        self.image_file = image_file\n",
    "        self.label_info = label_info\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index: the index of item\n",
    "        Returns:\n",
    "            image and its labels\n",
    "        \"\"\"\n",
    "        image_name = self.image_file[index].strip('\\n')\n",
    "        raw_label = self.label_info.loc[self.label_info['Image_index'] == image_name]\n",
    "        label = raw_label.iloc[:,0]\n",
    "        image_name = os.path.join(self.data_dir, image_name)\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_file)\n",
    "```\n",
    "\n",
    "构建好Dataset后，就可以使用DataLoader来按批次读入数据了，实现代码如下：\n",
    "\n",
    "```python\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=4, shuffle=False)\n",
    "```\n",
    "\n",
    "其中:\n",
    "\n",
    "- batch_size：样本是按“批”读入的，batch_size就是每次读入的样本数\n",
    "- num_workers：有多少个进程用于读取数据\n",
    "- shuffle：是否将读入的数据打乱\n",
    "- drop_last：对于样本最后一部分没有达到批次数的样本，使其不再参与训练\n",
    "\n",
    "这里可以看一下我们的加载的数据。PyTorch中的DataLoader的读取可以使用next和iter来完成\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "images, labels = next(iter(val_loader))\n",
    "print(images.shape)\n",
    "plt.imshow(images[0].transpose(1,2,0))\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "colored-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, info_csv, image_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: path to image directory.\n",
    "            info_csv: path to the csv file containing image indexes\n",
    "                with corresponding labels.\n",
    "            image_list: path to the txt file contains image names to training/validation set\n",
    "            transform: optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        label_info = pd.read_csv(info_csv)\n",
    "        image_file = open(image_list).readlines()\n",
    "        self.data_dir = data_dir\n",
    "        self.image_file = image_file\n",
    "        self.label_info = label_info\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index: the index of item\n",
    "        Returns:\n",
    "            image and its labels\n",
    "        \"\"\"\n",
    "        image_name = self.image_file[index].strip('\\n')\n",
    "        raw_label = self.label_info.loc[self.label_info['Image_index'] == image_name]\n",
    "        label = raw_label.iloc[:,0]\n",
    "        image_name = os.path.join(self.data_dir, image_name)\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mobile-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = Dataset.ImageFolder(train_path, transform=data_transform)\n",
    "# val_data = Dataset.ImageFolder(val_path, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "pediatric-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, shuffle=True, drop_last=True)\n",
    "# val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "functioning-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# images, labels = next(iter(val_loader))\n",
    "# print(images.shape)\n",
    "# plt.imshow(images[0].transpose(1,2,0))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-cinema",
   "metadata": {},
   "source": [
    "##模型构建\n",
    "\n",
    "#### 3.4.1  神经网络的构造\n",
    "\n",
    "PyTorch中神经网络构造一般是基于 Module 类的模型来完成的，它让模型构造更加灵活。\n",
    "\n",
    "Module 类是 nn 模块里提供的一个模型构造类，是所有神经⽹网络模块的基类，我们可以继承它来定义我们想要的模型。下面继承 Module 类构造多层感知机。这里定义的 MLP 类重载了 Module 类的 init 函数和 forward 函数。它们分别用于创建模型参数和定义前向计算。前向计算也即正向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "lucky-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  # 声明带有模型参数的层，这里声明了两个全连接层\n",
    "  def __init__(self, **kwargs):\n",
    "    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数\n",
    "    super(MLP, self).__init__(**kwargs)\n",
    "    self.hidden = nn.Linear(784, 256)\n",
    "    self.act = nn.ReLU()\n",
    "    self.output = nn.Linear(256,10)\n",
    "    \n",
    "   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\n",
    "  def forward(self, x):\n",
    "    o = self.act(self.hidden(x))\n",
    "    return self.output(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-chocolate",
   "metadata": {},
   "source": [
    "以上的 MLP 类中⽆须定义反向传播函数。系统将通过⾃动求梯度⽽自动⽣成反向传播所需的 backward 函数。\n",
    "\n",
    "我们可以实例化 MLP 类得到模型变量 net 。下⾯的代码初始化 net 并传入输⼊数据 X 做一次前向计算。其中， net(X) 会调用 MLP 继承⾃自 Module 类的 __call__ 函数，这个函数将调⽤用 MLP 类定义的forward 函数来完成前向计算。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "serious-sense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0776,  0.0266, -0.1749, -0.1583,  0.1085, -0.0408, -0.2163,  0.1046,\n",
       "         -0.0167, -0.0928],\n",
       "        [ 0.1790,  0.1069, -0.1989, -0.0214,  0.1270, -0.0847, -0.3163,  0.1285,\n",
       "         -0.0314, -0.1443]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2,784)\n",
    "net = MLP()\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-title",
   "metadata": {},
   "source": [
    "注意，这里并没有将 Module 类命名为 Layer (层)或者 Model (模型)之类的名字，这是因为该类是一个可供⾃由组建的部件。它的子类既可以是⼀个层(如PyTorch提供的 Linear 类)，⼜可以是一个模型(如这里定义的 MLP 类)，或者是模型的⼀个部分。\n",
    "\n",
    "#### 3.4.2  神经网络中常见的层\n",
    "\n",
    "深度学习的一个魅力在于神经网络中各式各样的层，例如全连接层、卷积层、池化层与循环层等等。虽然PyTorch提供了⼤量常用的层，但有时候我们依然希望⾃定义层。这里我们会介绍如何使用 Module 来自定义层，从而可以被反复调用。\n",
    "\n",
    "- **不含模型参数的层**\n",
    "\n",
    "我们先介绍如何定义一个不含模型参数的自定义层。下⾯构造的 MyLayer 类通过继承 Module 类自定义了一个**将输入减掉均值后输出**的层，并将层的计算定义在了 forward 函数里。这个层里不含模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "given-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "    def forward(self, x):\n",
    "        return x - x.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "medium-logic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试，实例化该层，然后做前向计算\n",
    "layer = MyLayer()\n",
    "layer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-actress",
   "metadata": {},
   "source": [
    "- **含模型参数的层**\n",
    "\n",
    "我们还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。\n",
    "\n",
    "Parameter 类其实是 Tensor 的子类，如果一 个 Tensor 是 Parameter ，那么它会⾃动被添加到模型的参数列表里。所以在⾃定义含模型参数的层时，我们应该将参数定义成 Parameter ，除了直接定义成 Parameter 类外，还可以使⽤ ParameterList 和 ParameterDict 分别定义参数的列表和字典。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ranging-conducting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyListDense(\n",
      "  (params): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyListDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyListDense, self).__init__()\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])\n",
    "        self.params.append(nn.Parameter(torch.randn(4, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.params)):\n",
    "            x = torch.mm(x, self.params[i])\n",
    "        return x\n",
    "net = MyListDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "thick-biology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDictDense(\n",
      "  (params): ParameterDict(\n",
      "      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyDictDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDictDense, self).__init__()\n",
    "        self.params = nn.ParameterDict({\n",
    "                'linear1': nn.Parameter(torch.randn(4, 4)),\n",
    "                'linear2': nn.Parameter(torch.randn(4, 1))\n",
    "        })\n",
    "        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # 新增\n",
    "\n",
    "    def forward(self, x, choice='linear1'):\n",
    "        return torch.mm(x, self.params[choice])\n",
    "\n",
    "net = MyDictDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-tobago",
   "metadata": {},
   "source": [
    "下面给出常见的神经网络的一些层，比如卷积层、池化层，以及较为基础的AlexNet，LeNet等。\n",
    "\n",
    "- **二维卷积层**\n",
    "\n",
    "二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "satisfied-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 卷积运算（二维互相关）\n",
    "def corr2d(X, K): \n",
    "    h, w = K.shape\n",
    "    X, K = X.float(), K.float()\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "# 二维卷积层\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-console",
   "metadata": {},
   "source": [
    "卷积窗口形状为 $p \\times q$ 的卷积层称为 $p \\times q$  卷积层。同样，  $p \\times q$ 卷积或 $p \\times q$ 卷积核说明卷积核的高和宽分别为 $p$ 和 $q$。\n",
    "\n",
    "填充(padding)是指在输⼊高和宽的两侧填充元素(通常是0元素)。\n",
    "\n",
    "下面的例子里我们创建一个⾼和宽为3的二维卷积层，然后设输⼊高和宽两侧的填充数分别为1。给定一 个高和宽为8的输入，我们发现输出的高和宽也是8。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "accepting-count",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 定义一个函数来计算卷积层。它对输入和输出做相应的升维和降维\n",
    "def comp_conv2d(conv2d, X):\n",
    "    # (1, 1)代表批量大小和通道数\n",
    "    X = X.view((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # 排除不关心的前两维:批量和通道\n",
    "    return Y.view(Y.shape[2:])\n",
    "\n",
    "# 注意这里是两侧分别填充1⾏或列，所以在两侧一共填充2⾏或列\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3,padding=1)\n",
    "\n",
    "X = torch.rand(8, 8)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ruled-sellers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽。\n",
    "# 使用高为5、宽为3的卷积核。在⾼和宽两侧的填充数分别为2和1\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(5, 3), padding=(2, 1))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "graduate-mistress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下 的顺序，依次在输⼊数组上滑动。我们将每次滑动的行数和列数称为步幅(stride)。\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-freight",
   "metadata": {},
   "source": [
    "\n",
    "填充可以增加输出的高和宽。这常用来使输出与输入具有相同的高和宽。\n",
    "\n",
    "步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的 ( 为大于1的整数)。\n",
    "\n",
    "- **池化层**\n",
    "\n",
    "池化层每次对输入数据的一个固定形状窗口(⼜称池化窗口)中的元素计算输出。不同于卷积层里计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也 分别叫做最大池化或平均池化。在二维最⼤池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。当池化窗口滑动到某⼀位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。\n",
    "\n",
    "下面把池化层的前向计算实现在`pool2d`函数里。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "legislative-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "oriental-computer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]], dtype=torch.float)\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "possible-guess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X, (2, 2), 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-riverside",
   "metadata": {},
   "source": [
    "我们可以使用`torch.nn`包来构建神经网络。我们已经介绍了`autograd`包，`nn`包则依赖于`autograd`包来定义模型并对它们求导。一个`nn.Module`包含各个层和一个`forward(input)`方法，该方法返回`output`。\n",
    "\n",
    "#### 3.4.3  模型示例\n",
    "\n",
    "- **LeNet**\n",
    "\n",
    "![3.4.1](./figures/3.4.1.png)\n",
    "\n",
    "这是一个简单的前馈神经网络 (feed-forward network）（LeNet）。它接受一个输入，然后将它送入下一层，一层接一层的传递，最后给出输出。\n",
    "\n",
    "一个神经网络的典型训练过程如下：\n",
    "\n",
    "1. 定义包含一些可学习参数(或者叫权重）的神经网络\n",
    "2. 在输入数据集上迭代\n",
    "3. 通过网络处理输入\n",
    "4. 计算 loss (输出和正确答案的距离）\n",
    "5. 将梯度反向传播给网络的参数\n",
    "6. 更新网络的权重，一般使用一个简单的规则：`weight = weight - learning_rate * gradient`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "growing-protection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 输入图像channel：1；输出channel：6；5x5卷积核\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 2x2 Max pooling\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # 如果是方阵,则可以只使用一个数字进行定义\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # 除去批处理维度的其他所有维度\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "finnish-medium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# 我们只需要定义 `forward` 函数，`backward`函数会在使用`autograd`时自动定义，`backward`函数用来计算导数。我们可以在 `forward` 函数中使用任何针对张量的操作和计算。\n",
    "\n",
    "# 一个模型的可学习参数可以通过`net.parameters()`返回\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "adjustable-phone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1124, -0.1571,  0.1364, -0.0642, -0.0528, -0.1446, -0.0338, -0.1146,\n",
      "          0.0769, -0.0177]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 让我们尝试一个随机的 32x32 的输入。注意:这个网络 (LeNet）的期待输入是 32x32 的张量。如果使用 MNIST 数据集来训练这个网络，要把图片大小重新调整到 32x32。\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "polished-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清零所有参数的梯度缓存，然后进行随机梯度的反向传播：\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-masters",
   "metadata": {},
   "source": [
    "注意：`torch.nn`只支持小批量处理 (mini-batches）。整个 `torch.nn` 包只支持小批量样本的输入，不支持单个样本的输入。比如，`nn.Conv2d` 接受一个4维的张量，即`nSamples x nChannels x Height x Width `如果是一个单独的样本，只需要使用`input.unsqueeze(0)` 来添加一个“假的”批大小维度。\n",
    "\n",
    "- `torch.Tensor` - 一个多维数组，支持诸如`backward()`等的自动求导操作，同时也保存了张量的梯度。\n",
    "\n",
    "- `nn.Module `- 神经网络模块。是一种方便封装参数的方式，具有将参数移动到GPU、导出、加载等功能。\n",
    "\n",
    "- `nn.Parameter `- 张量的一种，当它作为一个属性分配给一个`Module`时，它会被自动注册为一个参数。\n",
    "\n",
    "- `autograd.Function` - 实现了自动求导前向和反向传播的定义，每个`Tensor`至少创建一个`Function`节点，该节点连接到创建`Tensor`的函数并对其历史进行编码。\n",
    "\n",
    "下面再介绍一个比较基础的案例AlexNet\n",
    "\n",
    "- **AlexNet**\n",
    "\n",
    "![3.4.2](./figures/3.4.2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "healthy-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2), # kernel_size, stride\n",
    "            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "            nn.Conv2d(96, 256, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2),\n",
    "            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。\n",
    "            # 前两个卷积层后不使用池化层来减小输入的高和宽\n",
    "            nn.Conv2d(256, 384, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "         # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*5*5, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "            nn.Linear(4096, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(img.shape[0], -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "certified-disposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=6400, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = AlexNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-classroom",
   "metadata": {},
   "source": [
    "##损失函数\n",
    "\n",
    "在深度学习广为使用的今天，我们可以在脑海里清晰的知道，一个模型想要达到很好的效果需要学习，也就是我们常说的训练。一个好的训练离不开优质的负反馈，这里的损失函数就是模型的负反馈。\n",
    "\n",
    "![](./figures/3.5.1lossfunciton.png)\n",
    "\n",
    "所以在PyTorch中，损失函数是必不可少的。它是数据输入到模型当中，产生的结果与真实标签的评价指标，我们的模型可以按照损失函数的目标来做出改进。\n",
    "\n",
    "下面我们将开始探索pytorch的所拥有的损失函数。这里将列出PyTorch中常用的损失函数（一般通过torch.nn调用），并详细介绍每个损失函数的功能介绍、数学公式和调用代码。当然，PyTorch的损失函数还远不止这些，在解决实际问题的过程中需要进一步探索、借鉴现有工作，或者设计自己的损失函数。\n",
    "\n",
    "\n",
    "\n",
    "#### 3.5.1  二分类交叉熵损失函数\n",
    "\n",
    "```python\n",
    "torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "\n",
    "**功能**：计算二分类任务时的交叉熵（Cross Entropy）函数。在二分类中，label是{0,1}。对于进入交叉熵函数的input为概率分布的形式。一般来说，input为sigmoid激活层的输出，或者softmax的输出。\n",
    "\n",
    "**主要参数**：\n",
    "\n",
    "`weight`:每个类别的loss设置权值\n",
    "\n",
    "`size_average`:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。\n",
    "\n",
    "`reduce`:数据类型为bool，为True时，loss的返回是标量。\n",
    "\n",
    "计算公式如下：\n",
    "$$\n",
    "\\ell(x, y)=\\left\\{\\begin{array}{ll}\n",
    "\\operatorname{mean}(L), & \\text { if reduction }=\\text { 'mean' } \\\\\n",
    "\\operatorname{sum}(L), & \\text { if reduction }=\\text { 'sum' }\n",
    "\\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "still-trace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7678, 1.0628, 2.0349], requires_grad=True)\n",
      "tensor([0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "print(input)\n",
    "print(target)\n",
    "output = loss(m(input), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "catholic-typing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCELoss损失函数的计算结果为 tensor(0.2666, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('BCELoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-analysis",
   "metadata": {},
   "source": [
    "#### 3.5.2  交叉熵损失函数\n",
    "\n",
    "```python\n",
    "torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
    "```\n",
    "\n",
    "**功能**：计算交叉熵函数\n",
    "\n",
    "**主要参数**：  \n",
    "\n",
    "`weight`:每个类别的loss设置权值。\n",
    "\n",
    "`size_average`:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。\n",
    "\n",
    "`ignore_index`:忽略某个类的损失函数。\n",
    "\n",
    "`reduce`:数据类型为bool，为True时，loss的返回是标量。\n",
    "\n",
    "计算公式如下：\n",
    "$$\n",
    "\\operatorname{loss}(x, \\text { class })=-\\log \\left(\\frac{\\exp (x[\\text { class }])}{\\sum_{j} \\exp (x[j])}\\right)=-x[\\text { class }]+\\log \\left(\\sum_{j} \\exp (x[j])\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "exotic-celebrity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5258,  0.1394,  0.8994,  0.0508, -1.1742],\n",
      "        [ 0.5163,  1.2460, -1.1871, -1.8844,  0.7281],\n",
      "        [ 0.4035,  0.3512, -0.1732,  0.2700, -0.9559]], requires_grad=True)\n",
      "tensor([3, 1, 4])\n",
      "tensor(1.7634, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(input)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-marketing",
   "metadata": {},
   "source": [
    "#### 3.5.3  L1损失函数\n",
    "\n",
    "```python\n",
    "torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "\n",
    "**功能：** 计算输出`y`和真实标签`target`之间的差值的绝对值。\n",
    "\n",
    "我们需要知道的是，`reduction`参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。\n",
    "sum：所有元素求和，返回标量。\n",
    "mean：加权平均，返回标量。 \n",
    "如果选择`none`，那么返回的结果是和输入元素相同尺寸的。默认计算方式是求平均。\n",
    "\n",
    "**计算公式如下：**\n",
    "$$\n",
    "L_{n} = |x_{n}-y_{n}|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ambient-peninsula",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5000, -0.0690, -0.3350, -1.6670, -0.0905],\n",
      "        [-0.7136,  0.4314,  1.2817, -0.9765, -1.1095],\n",
      "        [ 0.2777, -0.0694,  1.2643, -0.0483,  0.2949]], requires_grad=True)\n",
      "tensor([[-0.1159, -1.2980,  0.6128, -0.5248,  0.2209],\n",
      "        [ 1.4999, -1.0349, -1.1215,  0.5327, -0.1730],\n",
      "        [-0.8085,  0.9483, -0.8768, -1.8048, -0.0934]])\n",
      "L1损失函数的计算结果为 tensor(1.3289, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.L1Loss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "print(input)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('L1损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-shareware",
   "metadata": {},
   "source": [
    "#### 3.5.4  MSE损失函数\n",
    "```python\n",
    "torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "**功能：** 计算输出`y`和真实标签`target`之差的平方。\n",
    "\n",
    "和`L1Loss`一样，`MSELoss`损失函数中，`reduction`参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。\n",
    "sum：所有元素求和，返回标量。默认计算方式是求平均。\n",
    "\n",
    "**计算公式如下：**\n",
    "\n",
    "$$\n",
    "l_{n}=\\left(x_{n}-y_{n}\\right)^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "related-madrid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1798, -0.0432,  0.2070, -0.6493,  1.3385],\n",
      "        [-0.3960, -0.5431, -0.4457, -0.4783, -2.1990],\n",
      "        [-0.4434,  0.3962,  1.4811,  0.3002, -1.8707]], requires_grad=True)\n",
      "tensor([[-0.1060, -0.7905, -1.4250,  0.1494, -1.6710],\n",
      "        [ 1.0123,  0.9307,  0.5518,  0.6348,  0.1266],\n",
      "        [ 2.6338, -0.3374,  1.2583,  1.1947,  0.2921]])\n",
      "MSE损失函数的计算结果为 tensor(2.6888, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "print(input)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('MSE损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-details",
   "metadata": {},
   "source": [
    "#### 3.5.5  平滑L1 (Smooth L1)损失函数\n",
    "```python\n",
    "torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=1.0)\n",
    "```\n",
    "**功能：** L1的平滑输出，其功能是减轻离群点带来的影响\n",
    "\n",
    "`reduction`参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。\n",
    "sum：所有元素求和，返回标量。默认计算方式是求平均。\n",
    "\n",
    "**提醒：** 之后的损失函数中，关于`reduction` 这个参数依旧会存在。所以，之后就不再单独说明。\n",
    "\n",
    "**计算公式如下：**\n",
    "$$\n",
    "\\operatorname{loss}(x, y)=\\frac{1}{n} \\sum_{i=1}^{n} z_{i}\n",
    "$$\n",
    "其中，\n",
    "$$\n",
    "z_{i}=\\left\\{\\begin{array}{ll}\n",
    "0.5\\left(x_{i}-y_{i}\\right)^{2}, & \\text { if }\\left|x_{i}-y_{i}\\right|<1 \\\\\n",
    "\\left|x_{i}-y_{i}\\right|-0.5, & \\text { otherwise }\n",
    "\\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "acoustic-universal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7672, -0.3196,  0.0947,  0.1789,  0.1324],\n",
      "        [-1.0629, -1.0032,  0.3680, -0.7535, -0.9793],\n",
      "        [ 0.4223,  0.1438,  0.0963,  0.9450, -0.8456]], requires_grad=True)\n",
      "tensor([[-0.4131,  0.6129,  2.1607,  1.7367,  0.0192],\n",
      "        [-1.7636, -0.7424, -2.0905, -0.2236, -0.1123],\n",
      "        [-0.0560,  0.3320, -0.0105,  1.0980, -0.9615]])\n",
      "SmoothL1Loss损失函数的计算结果为 tensor(0.4553, grad_fn=<SmoothL1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.SmoothL1Loss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "print(input)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('SmoothL1Loss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "declared-affect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# **平滑L1与L1的对比**\n",
    "# 这里我们通过可视化两种损失函数曲线来对比平滑L1和L1两种损失函数的区别。\n",
    "\n",
    "inputs = torch.linspace(-10, 10, steps=5000)\n",
    "target = torch.zeros_like(inputs)\n",
    "\n",
    "loss_f_smooth = nn.SmoothL1Loss(reduction='none')\n",
    "loss_smooth = loss_f_smooth(inputs, target)\n",
    "loss_f_l1 = nn.L1Loss(reduction='none')\n",
    "loss_l1 = loss_f_l1(inputs,target)\n",
    "\n",
    "plt.plot(inputs.numpy(), loss_smooth.numpy(), label='Smooth L1 Loss')\n",
    "plt.plot(inputs.numpy(), loss_l1, label='L1 loss')\n",
    "plt.xlabel('x_i - y_i')\n",
    "plt.ylabel('loss value')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-evaluation",
   "metadata": {},
   "source": [
    "可以看出，对于`smoothL1`来说，在 0 这个尖端处，过渡更为平滑。\n",
    "\n",
    "#### 3.5.6  目标泊松分布的负对数似然损失\n",
    "```python\n",
    "torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')\n",
    "```\n",
    "**功能：** 泊松分布的负对数似然损失函数\n",
    "\n",
    "**主要参数：**\n",
    "\n",
    "`log_input`：输入是否为对数形式，决定计算公式。\n",
    "\n",
    "`full`：计算所有 loss，默认为 False。\n",
    "\n",
    "`eps`：修正项，避免 input 为 0 时，log(input) 为 nan 的情况。\n",
    "\n",
    "**数学公式：**\n",
    "\n",
    "- 当参数`log_input=True`：\n",
    "$$\n",
    "\\operatorname{loss}\\left(x_{n}, y_{n}\\right)=e^{x_{n}}-x_{n} \\cdot y_{n}\n",
    "$$\n",
    "\n",
    "\n",
    "- 当参数`log_input=False`：\n",
    "\n",
    "    $$\n",
    "    \\operatorname{loss}\\left(x_{n}, y_{n}\\right)=x_{n}-y_{n} \\cdot \\log \\left(x_{n}+\\text { eps }\\right)\n",
    "    $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "headed-jordan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7678, 1.0628, 2.0349], requires_grad=True)\n",
      "tensor([[ 0.4237,  0.2304],\n",
      "        [-1.1830, -0.9445],\n",
      "        [-0.3609,  1.4539],\n",
      "        [-1.3601,  2.1541],\n",
      "        [ 1.5953,  0.4600]])\n",
      "PoissonNLLLoss损失函数的计算结果为 tensor(2.1622, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.PoissonNLLLoss()\n",
    "log_input = torch.randn(5, 2, requires_grad=True)\n",
    "target = torch.randn(5, 2)\n",
    "print(input)\n",
    "print(target)\n",
    "output = loss(log_input, target)\n",
    "output.backward()\n",
    "\n",
    "print('PoissonNLLLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-madrid",
   "metadata": {},
   "source": [
    "#### 3.5.7  KL散度\n",
    "```python\n",
    "torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)\n",
    "```\n",
    "**功能：** 计算KL散度，也就是计算相对熵。用于连续分布的距离度量，并且对离散采用的连续输出空间分布进行回归通常很有用。\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "`reduction`：计算模式，可为 `none`/`sum`/`mean`/`batchmean`。\n",
    "\n",
    "    none：逐个元素计算。\n",
    "    \n",
    "    sum：所有元素求和，返回标量。\n",
    "    \n",
    "    mean：加权平均，返回标量。\n",
    "    \n",
    "    batchmean：batchsize 维度求平均值。\n",
    "\n",
    "**计算公式：**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{\\mathrm{KL}}(P, Q)=\\mathrm{E}_{X \\sim P}\\left[\\log \\frac{P(X)}{Q(X)}\\right] &=\\mathrm{E}_{X \\sim P}[\\log P(X)-\\log Q(X)] \\\\\n",
    "&=\\sum_{i=1}^{n} P\\left(x_{i}\\right)\\left(\\log P\\left(x_{i}\\right)-\\log Q\\left(x_{i}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ahead-premium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.3000, 0.2000],\n",
      "        [0.2000, 0.3000, 0.5000]])\n",
      "tensor([[0.9000, 0.0500, 0.0500],\n",
      "        [0.1000, 0.7000, 0.2000]])\n",
      "KLDivLoss损失函数的计算结果为 tensor(-0.3335)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/googler/.local/lib/python3.8/site-packages/torch/nn/functional.py:2741: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[0.5, 0.3, 0.2], [0.2, 0.3, 0.5]])\n",
    "target = torch.tensor([[0.9, 0.05, 0.05], [0.1, 0.7, 0.2]], dtype=torch.float)\n",
    "print(inputs)\n",
    "print(target)\n",
    "loss = nn.KLDivLoss()\n",
    "output = loss(inputs,target)\n",
    "\n",
    "print('KLDivLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-breathing",
   "metadata": {},
   "source": [
    "#### 3.5.8  MarginRankingLoss\n",
    "```python\n",
    "torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "**功能：** 计算两个向量之间的相似度，用于排序任务。该方法用于计算两组数据之间的差异。\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "`margin`：边界值，$$x_{1}$$ 与$$x_{2}$$ 之间的差异值。\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "**计算公式：**\n",
    "\n",
    "$$\n",
    "\\operatorname{loss}(x 1, x 2, y)=\\max (0,-y *(x 1-x 2)+\\operatorname{margin})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "likely-father",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4080, -0.0435,  1.0979], requires_grad=True) tensor([-0.3391,  0.4939,  1.9473], requires_grad=True) tensor([-1., -1., -1.])\n",
      "MarginRankingLoss损失函数的计算结果为 tensor(0., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MarginRankingLoss()\n",
    "input1 = torch.randn(3, requires_grad=True)\n",
    "input2 = torch.randn(3, requires_grad=True)\n",
    "target = torch.randn(3).sign()\n",
    "print(input1, input2, target)\n",
    "output = loss(input1, input2, target)\n",
    "output.backward()\n",
    "\n",
    "print('MarginRankingLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-potato",
   "metadata": {},
   "source": [
    "#### 3.5.9  多标签边界损失函数\n",
    "```python\n",
    "torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "\n",
    "**功能：** 对于多标签分类问题计算损失函数。\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "**计算公式：**\n",
    "$$\n",
    "\\operatorname{loss}(x, y)=\\sum_{i j} \\frac{\\max (0,1-x[y[j]]-x[i])}{x \\cdot \\operatorname{size}(0)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\text { 其中, } i=0, \\ldots, x \\cdot \\operatorname{size}(0), j=0, \\ldots, y \\cdot \\operatorname{size}(0), \\text { 对于所有的 } i \\text { 和 } j \\text {, 都有 } y[j] \\geq 0 \\text { 并且 }\\\\\n",
    "i \\neq y[j]\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "careful-stephen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9000, 0.2000, 0.4000, 0.8000]]) tensor([[ 3,  0, -1,  1]])\n",
      "MultiLabelMarginLoss损失函数的计算结果为 tensor(0.4500)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MultiLabelMarginLoss()\n",
    "x = torch.FloatTensor([[0.9, 0.2, 0.4, 0.8]])\n",
    "# for target y, only consider labels 3 and 0, not after label -1\n",
    "y = torch.LongTensor([[3, 0, -1, 1]])# 真实的分类是，第3类和第0类\n",
    "print(x, y)\n",
    "output = loss(x, y)\n",
    "\n",
    "print('MultiLabelMarginLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-liechtenstein",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.5.10  二分类损失函数\n",
    "```python\n",
    "torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')torch.nn.(size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "**功能：** 计算二分类的 logistic 损失。\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "**计算公式：**\n",
    "\n",
    "$$\n",
    "\\operatorname{loss}(x, y)=\\sum_{i} \\frac{\\log (1+\\exp (-y[i] \\cdot x[i]))}{x \\cdot \\operatorname{nelement}()}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\n",
    "\\text { 其中, } x . \\text { nelement() 为输入 } x \\text { 中的样本个数。注意这里 } y \\text { 也有 } 1 \\text { 和 }-1 \\text { 两种模式。 }\n",
    "\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "interstate-douglas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3000, 0.7000],\n",
      "        [0.5000, 0.5000]]) tensor([[-1.,  1.],\n",
      "        [ 1., -1.]])\n",
      "SoftMarginLoss损失函数的计算结果为 tensor(0.6764)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[0.3, 0.7], [0.5, 0.5]])  # 两个样本，两个神经元\n",
    "target = torch.tensor([[-1, 1], [1, -1]], dtype=torch.float)  # 该 loss 为逐个神经元计算，需要为每个神经元单独设置标签\n",
    "print(inputs, target)\n",
    "loss_f = nn.SoftMarginLoss()\n",
    "output = loss_f(inputs, target)\n",
    "\n",
    "print('SoftMarginLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-negotiation",
   "metadata": {},
   "source": [
    "#### 3.5.11  多分类的折页损失\n",
    "```python\n",
    "torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "**功能：** 计算多分类的折页损失\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "`p：`可选 1 或 2。\n",
    "\n",
    "`weight`：各类别的 loss 设置权值。\n",
    "\n",
    "`margin`：边界值\n",
    "\n",
    "\n",
    "**计算公式：**\n",
    "\n",
    "$$\n",
    "\\operatorname{loss}(x, y)=\\frac{\\sum_{i} \\max (0, \\operatorname{margin}-x[y]+x[i])^{p}}{x \\cdot \\operatorname{size}(0)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\text { 其中, } x \\in\\{0, \\ldots, x \\cdot \\operatorname{size}(0)-1\\}, y \\in\\{0, \\ldots, y \\cdot \\operatorname{size}(0)-1\\} \\text {, 并且对于所有的 } i \\text { 和 } j \\text {, }\\\\\n",
    "\\text { 都有 } 0 \\leq y[j] \\leq x \\cdot \\operatorname{size}(0)-1, \\text { 以及 } i \\neq y[j] \\text { 。 }\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fixed-brazil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3000, 0.7000],\n",
      "        [0.5000, 0.5000]]) tensor([0, 1])\n",
      "MultiMarginLoss损失函数的计算结果为 tensor(0.6000)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[0.3, 0.7], [0.5, 0.5]]) \n",
    "target = torch.tensor([0, 1], dtype=torch.long) \n",
    "\n",
    "loss_f = nn.MultiMarginLoss()\n",
    "output = loss_f(inputs, target)\n",
    "print(inputs ,target)\n",
    "print('MultiMarginLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-terror",
   "metadata": {},
   "source": [
    "#### 3.5.12  三元组损失\n",
    "\n",
    "```python\n",
    "torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "**功能：** 计算三元组损失。\n",
    "\n",
    "**三元组:** 这是一种数据的存储或者使用格式。<实体1，关系，实体2>。在项目中，也可以表示为< `anchor`, `positive examples` , `negative examples`>\n",
    "\n",
    "在这个损失函数中，我们希望去`anchor`的距离更接近`positive examples`，而远离`negative examples `\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "`p：`可选 1 或 2。\n",
    "\n",
    "\n",
    "`margin`：边界值\n",
    "\n",
    "\n",
    "**计算公式：**\n",
    "\n",
    "$$\n",
    "L(a, p, n)=\\max \\left\\{d\\left(a_{i}, p_{i}\\right)-d\\left(a_{i}, n_{i}\\right)+\\operatorname{margin}, 0\\right\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { 其中, } d\\left(x_{i}, y_{i}\\right)=\\left\\|\\mathbf{x}_{i}-\\mathbf{y}_{i}\\right\\|_{\\text {・ }}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "stainless-concept",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3521, -1.2300,  0.1997,  ...,  0.1583, -0.7949,  0.1089],\n",
      "        [-1.7695, -0.6294,  1.6083,  ..., -0.1657,  0.1117,  0.0486],\n",
      "        [-0.9730, -0.6594,  0.3879,  ...,  0.7471,  0.2908, -0.7662],\n",
      "        ...,\n",
      "        [-0.2888, -0.9644, -0.7746,  ..., -0.6941,  0.4096, -1.2438],\n",
      "        [ 0.7742,  1.2561,  1.9119,  ..., -0.7570,  0.4977,  0.2828],\n",
      "        [-2.4217, -0.4717,  1.1052,  ...,  0.8187,  0.7275,  0.8578]],\n",
      "       requires_grad=True) tensor([[-0.1668, -0.5596, -0.3265,  ...,  0.6739,  1.2366, -1.5409],\n",
      "        [ 0.8056, -2.0447,  0.2014,  ..., -0.0702, -0.9089,  0.6037],\n",
      "        [-1.1150, -0.1481,  1.0329,  ..., -0.0888,  1.3931,  0.7356],\n",
      "        ...,\n",
      "        [ 0.1900,  0.0067,  0.4947,  ..., -0.0135, -0.3680, -0.0849],\n",
      "        [ 1.0514, -0.4749,  0.5712,  ...,  1.1862,  1.8020, -0.2368],\n",
      "        [ 1.1421, -0.9168, -1.0399,  ..., -1.0033,  0.9503, -1.3986]],\n",
      "       requires_grad=True) tensor([[-0.4625,  0.7936, -0.7085,  ..., -0.5274,  0.2260, -0.4715],\n",
      "        [-2.8314, -0.4262,  0.8281,  ...,  1.3563, -1.3751, -1.3992],\n",
      "        [-0.4679, -0.2383,  1.0658,  ...,  0.8329,  0.9436,  1.9580],\n",
      "        ...,\n",
      "        [ 2.0819, -0.5634,  0.7953,  ...,  1.1527,  0.7805,  0.7877],\n",
      "        [-0.2244, -0.2020,  0.3195,  ..., -0.2371, -1.1041, -0.4534],\n",
      "        [-1.3706,  0.2575,  0.0254,  ..., -0.8397, -0.4970, -1.7352]],\n",
      "       requires_grad=True)\n",
      "TripletMarginLoss损失函数的计算结果为 tensor(1.0384, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "anchor = torch.randn(100, 128, requires_grad=True)\n",
    "positive = torch.randn(100, 128, requires_grad=True)\n",
    "negative = torch.randn(100, 128, requires_grad=True)\n",
    "print(anchor, positive, negative)\n",
    "output = triplet_loss(anchor, positive, negative)\n",
    "output.backward()\n",
    "print('TripletMarginLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-launch",
   "metadata": {},
   "source": [
    "#### 3.5.13  HingEmbeddingLoss\n",
    "```python\n",
    "torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "**功能：** 对输出的embedding结果做Hing损失计算\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "\n",
    "\n",
    "`margin`：边界值\n",
    "\n",
    "\n",
    "**计算公式：**\n",
    "\n",
    "$$\n",
    "l_{n}=\\left\\{\\begin{array}{ll}\n",
    "x_{n}, & \\text { if } y_{n}=1 \\\\\n",
    "\\max \\left\\{0, \\Delta-x_{n}\\right\\}, & \\text { if } y_{n}=-1\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "**注意事项：** 输入x应为两个输入之差的绝对值。\n",
    "\n",
    "可以这样理解，让个输出的是正例yn=1,那么loss就是x，如果输出的是负例y=-1，那么输出的loss就是要做一个比较。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "analyzed-mainland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.8000, 0.5000]]) tensor([[ 1,  1, -1]])\n",
      "HingEmbeddingLoss损失函数的计算结果为 tensor(0.7667)\n"
     ]
    }
   ],
   "source": [
    "loss_f = nn.HingeEmbeddingLoss()\n",
    "inputs = torch.tensor([[1., 0.8, 0.5]])\n",
    "target = torch.tensor([[1, 1, -1]])\n",
    "print(inputs, target)\n",
    "output = loss_f(inputs,target)\n",
    "\n",
    "print('HingEmbeddingLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-marathon",
   "metadata": {},
   "source": [
    "#### 3.5.14  余弦相似度\n",
    "```python\n",
    "torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "**功能：** 对两个向量做余弦相似度\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "\n",
    "\n",
    "`margin`：可取值[-1,1] ，推荐为[0,0.5] 。\n",
    "\n",
    "\n",
    "**计算公式：**\n",
    "\n",
    "$$\n",
    "\\operatorname{loss}(x, y)=\\left\\{\\begin{array}{ll}\n",
    "1-\\cos \\left(x_{1}, x_{2}\\right), & \\text { if } y=1 \\\\\n",
    "\\max \\left\\{0, \\cos \\left(x_{1}, x_{2}\\right)-\\text { margin }\\right\\}, & \\text { if } y=-1\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "其中,\n",
    "$$\n",
    "\\cos (\\theta)=\\frac{A \\cdot B}{\\|A\\|\\|B\\|}=\\frac{\\sum_{i=1}^{n} A_{i} \\times B_{i}}{\\sqrt{\\sum_{i=1}^{n}\\left(A_{i}\\right)^{2}} \\times \\sqrt{\\sum_{i=1}^{n}\\left(B_{i}\\right)^{2}}}\n",
    "$$\n",
    "\n",
    "\n",
    "这个损失函数应该是最广为人知的。对于两个向量，做余弦相似度。将余弦相似度作为一个距离的计算方式，如果两个向量的距离近，则损失函数值小，反之亦然。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "outside-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_f = nn.CosineEmbeddingLoss()\n",
    "# inputs_1 = torch.tensor([[0.3, 0.5, 0.7], [0.3, 0.5, 0.7]])\n",
    "# inputs_2 = torch.tensor([[0.1, 0.3, 0.5], [0.1, 0.3, 0.5]])\n",
    "# target = torch.tensor([[1, -1]], dtype=torch.float)\n",
    "# # print(inputs_1, inputs_2, target)\n",
    "# output = loss_f(inputs_1, inputs_2, target)\n",
    "\n",
    "# nn.CosineEmbeddingLoss()\n",
    "\n",
    "# print('CosineEmbeddingLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-communist",
   "metadata": {},
   "source": [
    "#### 3.5.15  CTC损失函数\n",
    "```python\n",
    "torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)\n",
    "```\n",
    "**功能：** 用于解决时序类数据的分类\n",
    "\n",
    "计算连续时间序列和目标序列之间的损失。CTCLoss对输入和目标的可能排列的概率进行求和，产生一个损失值，这个损失值对每个输入节点来说是可分的。输入与目标的对齐方式被假定为 \"多对一\"，这就限制了目标序列的长度，使其必须是≤输入长度。\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "\n",
    "`blank`：blank label。\n",
    "\n",
    "\n",
    "`zero_infinity`：无穷大的值或梯度值为 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "russian-implementation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTCLoss损失函数的计算结果为 tensor(28.6589, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Target are to be padded\n",
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "N = 16      # Batch size\n",
    "S = 30      # Target sequence length of longest target in batch (padding length)\n",
    "S_min = 10  # Minimum target length, for demonstration purposes\n",
    "\n",
    "# Initialize random batch of input vectors, for *size = (T,N,C)\n",
    "input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n",
    "\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "# Target are to be un-padded\n",
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "N = 16      # Batch size\n",
    "\n",
    "# Initialize random batch of input vectors, for *size = (T,N,C)\n",
    "input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)\n",
    "target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()\n",
    "\n",
    "print('CTCLoss损失函数的计算结果为',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-oasis",
   "metadata": {},
   "source": [
    "#### 3.6.2  Pytorch提供的优化器\n",
    "\n",
    "Pytorch很人性化的给我们提供了一个优化器的库torch.optim，在这里面提供了十种优化器。\n",
    "\n",
    "+ torch.optim.ASGD\n",
    "+ torch.optim.Adadelta\n",
    "+ torch.optim.Adagrad\n",
    "+ torch.optim.Adam\n",
    "+ torch.optim.AdamW\n",
    "+ torch.optim.Adamax\n",
    "+ torch.optim.LBFGS\n",
    "+ torch.optim.RMSprop\n",
    "+ torch.optim.Rprop\n",
    "+ torch.optim.SGD\n",
    "+ torch.optim.SparseAdam\n",
    "\n",
    "而以上这些优化算法均继承于`Optimizer`，下面我们先来看下所有优化器的基类`Optimizer`。定义如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aboriginal-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, params, defaults):        \n",
    "        self.defaults = defaults\n",
    "        self.state = defaultdict(dict)\n",
    "        self.param_groups = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-experiment",
   "metadata": {},
   "source": [
    "**`Optimizer`有三个属性：**\n",
    "\n",
    "+ `defaults`：存储的是优化器的超参数，例子如下：\n",
    "\n",
    "```python\n",
    "{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}\n",
    "```\n",
    "\n",
    "+ `state`：参数的缓存，例子如下：\n",
    "\n",
    "```python\n",
    "defaultdict(<class 'dict'>, {tensor([[ 0.3864, -0.0131],\n",
    "        [-0.1911, -0.4511]], requires_grad=True): {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
    "        [0.0052, 0.0052]])}})\n",
    "```\n",
    "\n",
    "+ `param_groups`：管理的参数组，是一个list，其中每个元素是一个字典，顺序是params，lr，momentum，dampening，weight_decay，nesterov，例子如下：\n",
    "\n",
    "```python\n",
    "[{'params': [tensor([[-0.1022, -1.6890],[-1.5116, -1.7846]], requires_grad=True)], 'lr': 1, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}]\n",
    "```\n",
    "\n",
    "**`Optimizer`还有以下的方法：**\n",
    "\n",
    "+ `zero_grad()`：清空所管理参数的梯度，PyTorch的特性是张量的梯度不自动清零，因此每次反向传播后都需要清空梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "opponent-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(self, set_to_none: bool = False):\n",
    "    for group in self.param_groups:\n",
    "        for p in group['params']:\n",
    "            if p.grad is not None:  #梯度不为空\n",
    "                if set_to_none: \n",
    "                    p.grad = None\n",
    "                else:\n",
    "                    if p.grad.grad_fn is not None:\n",
    "                        p.grad.detach_()\n",
    "                    else:\n",
    "                        p.grad.requires_grad_(False)\n",
    "                    p.grad.zero_()# 梯度设置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dated-scientist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data of weight before step:\n",
      "tensor([[-0.6408,  0.4553],\n",
      "        [-0.6091,  0.3922]])\n",
      "The grad of weight before step:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "The data of weight after step:\n",
      "tensor([[-0.7408,  0.3553],\n",
      "        [-0.7091,  0.2922]])\n",
      "The grad of weight after step:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "The grad of weight after optimizer.zero_grad():\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "optimizer.params_group is \n",
      "[{'params': [tensor([[-0.7408,  0.3553],\n",
      "        [-0.7091,  0.2922]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}]\n",
      "weight in optimizer:140360452351296\n",
      "weight in weight:140360452351296\n",
      "\n",
      "optimizer.param_groups is\n",
      "[{'params': [tensor([[-0.7408,  0.3553],\n",
      "        [-0.7091,  0.2922]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}, {'params': [tensor([[ 0.5173,  0.0395,  0.0376],\n",
      "        [-1.1254,  0.5430, -0.5823],\n",
      "        [ 0.7650, -1.3181, -2.3473]], requires_grad=True)], 'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0}]\n",
      "state_dict before step:\n",
      " {'state': {0: {'momentum_buffer': tensor([[1., 1.],\n",
      "        [1., 1.]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n",
      "state_dict after step:\n",
      " {'state': {0: {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
      "        [0.0052, 0.0052]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n",
      "----------done-----------\n",
      "load state_dict successfully\n",
      "{'state': {0: {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
      "        [0.0052, 0.0052]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n",
      "\n",
      "{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}\n",
      "\n",
      "defaultdict(<class 'dict'>, {tensor([[-1.6362, -0.5401],\n",
      "        [-1.6045, -0.6031]], requires_grad=True): {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
      "        [0.0052, 0.0052]])}})\n",
      "\n",
      "[{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [tensor([[-1.6362, -0.5401],\n",
      "        [-1.6045, -0.6031]], requires_grad=True)]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [tensor([[ 0.5173,  0.0395,  0.0376],\n",
      "        [-1.1254,  0.5430, -0.5823],\n",
      "        [ 0.7650, -1.3181, -2.3473]], requires_grad=True)]}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 设置权重，服从正态分布  --> 2 x 2\n",
    "weight = torch.randn((2, 2), requires_grad=True)\n",
    "# 设置梯度为全1矩阵  --> 2 x 2\n",
    "weight.grad = torch.ones((2, 2))\n",
    "# 输出现有的weight和data\n",
    "print(\"The data of weight before step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight before step:\\n{}\".format(weight.grad))\n",
    "# 实例化优化器\n",
    "optimizer = torch.optim.SGD([weight], lr=0.1, momentum=0.9)\n",
    "# 进行一步操作\n",
    "optimizer.step()\n",
    "# 查看进行一步后的值，梯度\n",
    "print(\"The data of weight after step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight after step:\\n{}\".format(weight.grad))\n",
    "# 权重清零\n",
    "optimizer.zero_grad()\n",
    "# 检验权重是否为0\n",
    "print(\"The grad of weight after optimizer.zero_grad():\\n{}\".format(weight.grad))\n",
    "# 输出参数\n",
    "print(\"optimizer.params_group is \\n{}\".format(optimizer.param_groups))\n",
    "# 查看参数位置，optimizer和weight的位置一样，我觉得这里可以参考Python是基于值管理\n",
    "print(\"weight in optimizer:{}\\nweight in weight:{}\\n\".format(id(optimizer.param_groups[0]['params'][0]), id(weight)))\n",
    "# 添加参数：weight2\n",
    "weight2 = torch.randn((3, 3), requires_grad=True)\n",
    "optimizer.add_param_group({\"params\": weight2, 'lr': 0.0001, 'nesterov': True})\n",
    "# 查看现有的参数\n",
    "print(\"optimizer.param_groups is\\n{}\".format(optimizer.param_groups))\n",
    "# 查看当前状态信息\n",
    "opt_state_dict = optimizer.state_dict()\n",
    "print(\"state_dict before step:\\n\", opt_state_dict)\n",
    "# 进行5次step操作\n",
    "for _ in range(50):\n",
    "    optimizer.step()\n",
    "# 输出现有状态信息\n",
    "print(\"state_dict after step:\\n\", optimizer.state_dict())\n",
    "# 保存参数信息\n",
    "torch.save(optimizer.state_dict(),os.path.join(r\"./\", \"optimizer_state_dict.pkl\"))\n",
    "print(\"----------done-----------\")\n",
    "# 加载参数信息\n",
    "state_dict = torch.load(r\"./optimizer_state_dict.pkl\") # 需要修改为你自己的路径\n",
    "optimizer.load_state_dict(state_dict)\n",
    "print(\"load state_dict successfully\\n{}\".format(state_dict))\n",
    "# 输出最后属性信息\n",
    "print(\"\\n{}\".format(optimizer.defaults))\n",
    "print(\"\\n{}\".format(optimizer.state))\n",
    "print(\"\\n{}\".format(optimizer.param_groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-greene",
   "metadata": {},
   "source": [
    "#### 3.6.5  实验\n",
    "\n",
    "为了更好的帮大家了解优化器，我们对PyTorch中的优化器进行了一个小测试\n",
    "\n",
    "**数据生成**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "settled-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(-1, 1, 1000)\n",
    "# 升维操作\n",
    "x = torch.unsqueeze(a, dim=1)\n",
    "y = x.pow(2) + 0.1 * torch.normal(torch.zeros(x.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "consolidated-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = nn.Linear(1, 20)\n",
    "        self.predict = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.predict(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-chain",
   "metadata": {},
   "source": [
    "下面这部分是测试图，纵坐标代表Loss，横坐标代表的是Step：\n",
    "\n",
    "![](./figures/3.6.2.png)\n",
    "\n",
    "在上面的图片上，曲线下降的趋势和对应的steps代表了在这轮数据，模型下的收敛速度\n",
    "\n",
    "**注意:**\n",
    "\n",
    "优化器的选择是需要根据模型进行改变的，不存在绝对的好坏之分，我们需要多进行一些测试。\n",
    "\n",
    "后续会添加SparseAdam，LBFGS这两个优化器的可视化结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-theme",
   "metadata": {},
   "source": [
    "### 训练和评估\n",
    "\n",
    "完成了上述设定后就可以加载数据开始训练模型了。首先应该设置模型的状态：如果是训练状态，那么模型的参数应该支持反向传播的修改；如果是验证/测试状态，则不应该修改模型参数。在PyTorch中，模型的状态设置非常简便，如下的两个操作二选一即可：\n",
    "\n",
    "```python\n",
    "model.train()   # 训练状态\n",
    "model.eval()   # 验证/测试状态\n",
    "```\n",
    "\n",
    "我们前面在DataLoader构建完成后介绍了如何从中读取数据，在训练过程中使用类似的操作即可，区别在于此时要用for循环读取DataLoader中的全部数据。\n",
    "\n",
    "```python\n",
    "for data, label in train_loader:\n",
    "```\n",
    "\n",
    "之后将数据放到GPU上用于后续计算，此处以.cuda()为例\n",
    "\n",
    "```python\n",
    "data, label = data.cuda(), label.cuda()\n",
    "```\n",
    "\n",
    "开始用当前批次数据做训练时，应当先将优化器的梯度置零：\n",
    "\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "之后将data送入模型中训练：\n",
    "\n",
    "```python\n",
    "output = model(data)\n",
    "```\n",
    "\n",
    "根据预先定义的criterion计算损失函数：\n",
    "\n",
    "```python\n",
    "loss = criterion(output, label)\n",
    "```\n",
    "\n",
    "将loss反向传播回网络：\n",
    "\n",
    "```python\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "使用优化器更新模型参数：\n",
    "\n",
    "```python\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "这样一个训练过程就完成了，后续还可以计算模型准确率等指标，这部分会在下一节的图像分类实战中加以介绍。\n",
    "\n",
    "验证/测试的流程基本与训练过程一致，不同点在于：\n",
    "\n",
    "- 需要预先设置torch.no_grad，以及将model调至eval模式\n",
    "- 不需要将优化器的梯度置零\n",
    "- 不需要将loss反向回传到网络\n",
    "- 不需要更新optimizer\n",
    "\n",
    "一个完整的训练过程如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "extended-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.cuda(), label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(label, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "electric-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对应的，一个完整的验证过程如下所示：\n",
    "def val(epoch):       \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in val_loader:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            output = model(data)\n",
    "            preds = torch.argmax(output, 1)\n",
    "            loss = criterion(output, label)\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "            running_accu += torch.sum(preds == label.data)\n",
    "    val_loss = val_loss/len(val_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-treasurer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
